{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7ff7fc",
   "metadata": {},
   "source": [
    "## 正常"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccca5a9",
   "metadata": {},
   "source": [
    "### 微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba733da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hcgf\n",
    "gl = hcgf.GlmLora(\"THUDM/chatglm-6b\", device=\"cuda:0\", lora_r=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e75df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "gl.load_data(\"./data/chatgpt_finetune_faq.json\").tune()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925aa224",
   "metadata": {},
   "source": [
    "### 推理\n",
    "\n",
    "重启下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08992e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hcgf\n",
    "gl = hcgf.GlmLora(\"THUDM/chatglm-6b\", device=\"cuda:1\", infer_mode=True, lora_r=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5594ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gl.load_pretrained(\"../souche-ai-chatgpt/src/models/lora-best.pt\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ca979",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"你是哪位？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ddbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gl.chat(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4400f7ef",
   "metadata": {},
   "source": [
    "## 8bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd758105",
   "metadata": {},
   "source": [
    "### 微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60789e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hcgf\n",
    "gl = hcgf.GlmLora(\"THUDM/chatglm-6b\", load_in_8bit=True, lora_r=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac8c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "gl.load_data(\"./data/chatgpt_finetune_faq.json\").tune()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c72d70",
   "metadata": {},
   "source": [
    "### 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a083ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 111\n",
      "CUDA SETUP: Loading binary /home/wyd/miniconda3/envs/flexgen/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyd/miniconda3/envs/flexgen/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /home/wyd/miniconda3/envs/flexgen did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/wyd/miniconda3/envs/flexgen/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/local/lib:/usr/lib:/usr/local/lib64:/usr/lib64 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/wyd/miniconda3/envs/flexgen/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model of THUDM/chatglm-6b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1114ff6ca06840f88e8d8334d3493089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing peft model\n",
      "trainable params: 14680064 || all params: 6269886464 \n",
      "trainable%: 0.234136041924985\n"
     ]
    }
   ],
   "source": [
    "import hcgf\n",
    "gl = hcgf.GlmLora(\"THUDM/chatglm-6b\", load_in_8bit=True, infer_mode=True, lora_r=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7283271",
   "metadata": {},
   "outputs": [],
   "source": [
    "gl.load_pretrained(\"./output/lora-ckpt-best-101280.pt\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ddd9083",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个名为 ChatGLM-6B 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。\n"
     ]
    }
   ],
   "source": [
    "inp = \"你是谁？\"\n",
    "history = []\n",
    "response, history = gl.chat(inp, history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ab1969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
